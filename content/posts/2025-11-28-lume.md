---
title: Lume
description: The MIDI-powered audio visualizer that works directly within the browser.
date: 2025-11-28
category: deep dive
---
Audio visualization is a nice thing I like to have as part of my music. Whenever I post a song on YouTube, I’ll usually slap a spectrum visualizer so viewers (including me) have something to see while people listen. I think the most prominent example are those EDM YouTube promoters/publishers, like [Proximity](https://www.youtube.com/watch?v=3Ex1sktR7aM) and [Monstercat](https://www.youtube.com/watch?v=_cB3HXVvm0g).

Lume is a project that was inspired by a few things, but the root of it was from a tweet from a music producer named symmez.

<blockquote class="twitter-tweet" data-lang="en" data-theme="dark"><p lang="ja" dir="ltr">p5.jsの練習でコードの視覚化をしました <a href="[https://t.co/6vt8K7bOIr">pic.twitter.com/6vt8K7bOIr</a></p>—](https://t.co/6vt8K7bOIr">pic.twitter.com/6vt8K7bOIr</a></p>—) 新目鳥 (@Symmez) <a href="[https://twitter.com/Symmez/status/1855265702799925325?ref\_src=twsrc^tfw">November](https://twitter.com/Symmez/status/1855265702799925325?ref_src=twsrc^tfw">November) 9, 2024</a></blockquote> <script async src="[https://platform.twitter.com/widgets.js](https://platform.twitter.com/widgets.js)" charset="utf-8"></script>

He had posted a little behind-the-scenes of one of his visualizers, where half of the screen had the code used to powered it. In a reply, he mentioned that he used p5.js, a JavaScript library that made it easier to make visuals.

I thought it was pretty cool, and I’ve always wanted to work on a JavaScript\* project that wasn’t strictly front-end, so I tried to make something based off of it— with my own flair, of course.

\*I swapped to TypeScript after two days of progress. I was working on a function that needed numerical inputs and I was like “do I have to verify the types for everything now? i wish javascript had types”

“wait.”

## midi?

The main thing to probably mention is that despite my wording, Lume is technically not listening to anything to generate visuals, instead using MIDI. If you have no idea what that is, you can think of the difference between MIDI and audio to be the same as the difference between sheet music and someone performing the music.

Outside of my direct inspiration, MIDI is a more sensible choice because the _notes_ are what I want to render, not the frequencies themselves. There’s no easy way to cleanly get note data from an audio file, and if you were… you’d just be converting it into a format similar to MIDI.